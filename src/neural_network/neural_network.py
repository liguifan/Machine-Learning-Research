'''
Module to construct a fully connected neural network.
'''
import argparse
import copy
import cPickle as pickle
import numpy as np

from activation_functions import get_actv_func
from cost_function import logistic_cost

class network:
    def __init__(self, actv_func):
        self.layer_weights = []
        self.actv, self.actv_der = get_actv_func(actv_func)

    def __random_init(self, n_features, n_classes, hidden_units):
        '''
        Randomly initialize the weights of neural network between [-0.5, 0.5).
        ''' 
        self.layer_weights.append(np.random.rand(hidden_units, n_features) -
                0.5) 

        col_inds = np.arange(hidden_units).reshape((n_classes,-1),
                order='F').reshape((-1,)) 
        row_inds =np.tile(np.arange(n_classes),(hidden_units/n_classes,
                1)).T.reshape(-1) 
        weights = np.zeros((n_classes, hidden_units))
        weights[row_inds, col_inds] = np.random.rand(hidden_units) - 0.5

        self.layer_weights.append(weights)

    def __get_indicator_vector(self, Y):
        '''
        convert Y into a matrix of indicator vectors
        '''
        n_classes = np.unique(Y).size
        new_Y = np.zeros((Y.size, n_classes), dtype=int)
        new_Y[np.array(range(Y.size)), Y] = 1
        return Y

    def __extend_for_bias(self, X):
        '''
        Extend the feature matrix to add bias.
        '''
        X = np.concatenate((np.ones(X.shape[0])[:, np.newaxis], X), axis=1)
        return X

    def __weights_init(self, layer_weights):
        '''
        Initialize the weights of the neural network with the given weights.
        '''
        self.layer_weights = layer_weights

    def __fwd_prop(self, x):
        '''
        Forward propagate the input value with current weights and return
        activations at each stage. The first activation entry is always the
        input itself.
        '''
        activation = [x]
        # calculate activations at hidden layer and output layers
        for ind, l_wt in enumerate(self.layer_weights):
            activation.append(self.actv(np.dot(l_wt, activation[ind]))) 

        return activation
             
    def __back_prop(self, activation, y):
        '''
        Calculates final error and back propogate it to all layers. Use
        activations generated by the forward propogation under the current
        weights. 

        Returns errors on all the hidden layers and output layer in that order.
        '''
        n_layers = len(self.layer_weights) + 1
        errors = [activation[-1] - y]

        # compute errors on hidden layers from o/p to i/p direction
        for ind in range(n_layers - 2):
            layer_ind = -(1 + ind)
            wt = self.layer_weights[layer_ind]
            act = activation[layer_ind - 1]
            next_layer_err = errors[-1]
            errors.append(np.multiply(np.dot(wt.T, next_layer_err),
                self.actv_der(act))) 
        errors.reverse()
        return errors

    def __derivatives(self, x, y):
        '''
        Calculates the partial derivatives at given x and y using fwd and back
        propagation.

        Return partial derivatives for the weight matrices.
        '''
        activation = self.__fwd_prop(x)
        deltas = self.__back_prop(activation, y)
        p_derivs = [np.outer(deltas[layer], activation[layer]) for layer in
                range(0, len(activation) - 1)] 
        return p_derivs

    def check_gradient(self, X, Y):
        '''
        Checks gradients computed by fwd/back prop. Return True if the gradients
        computed by back_prop are close the numerically computed gradient by
        0.001; else return False.
        '''
        # calculate gradient numerically
        EPS = 10e-4
        grad = [np.empty_like(weights) for weights in self.layer_weights]
        for layer in range(len(self.layer_weights)): 
            for (x,y), _ in np.nd_enumerate(self.layer_weight[layer]):
                layer_wts_cp = copy.deepcopy(self.layer_weights) 
                layer_wts_cp[layer][x][y] += EPS
                cost_1 = logistic_cost(layer_wts_cp)
                layer_wts_cp[layer][x][x] -= 2 * EPS
                cost_2 = logistic_cost(layer_wts_cp)
                grad[layer][x][y] = (cost_1 - cost_2) / 2 * EPS 

        # calculate gradient using back propagation
        X = self.__extend_for_bias(X)
        Y = self.__get_indicator_vector(Y)
        r, _ = X.shape

        # calculate gradient using back propagation
        derv = [np.zeros_like(weights) for weights in self.layer_weights]
        for row in range(r):
            derv_c = self.__derivatives(X[row], Y[row])
            for i in range(len(derv)): derv[i] += derv_c[i]

        diff = [np.amax(np.absolute(gradl - dervl)) for gradl, dervl in
                zip(grad, derv)]
        print max(diff)
        return max(diff) < 0.001

    def __update_weights(self, p_derivs, learning_rate):
        '''
        Updates the current weights using the given partial derivatives and the
        learning rate.
        '''
        for layer in range(len(self.layer_weights)): 
            self.layer_weights[layer] -=  p_derivs[layer]

    def __sgd(self, X, Y, epochs = 70000, learning_rate = 1.0):
        '''
        Performs stochastic gradient descent on the dataset X,Y for the given
        number of epochs using the given learning rate.
        '''
        for ind in range(X.shape[0]):
           p_derivs = self.__derivatives(X[ind], Y[ind],)
           self.__update_weights(p_derivs, learning_rate)
           if ind%100 == 0:
               print "Iterations completed: ", ind + 1

    def train(self, X, Y, hidden_units, layer_weights = None):
        '''
        Trains the network using Stochastic Gradient Descent. Initialize the
        network with the provided layer_weights. If no layer_weights are
        provided use random weights to initialize the network. Also, the
        training data is assumed to be randomly shuffled already.
        '''
        X = self.__extend_for_bias(X)
        Y = self.__get_indicator_vector(Y)
        n_examples, n_features = X.shape
        n_classes, _ = Y.shape

        # initialize network
        if layer_weights == None:self.__random_init(n_features, n_classes,
                hidden_units)
        else: self.__weights_init(layer_weights)

        # train
        self.__sgd(X,Y)
        
    def predict(self, X):
        '''
        Predict the classes based on the learned model and measure accuracy. 
        '''
        # add extra feature for bias
        X = self.__extend_for_bias(X)
        r, _ = X.shape
        pred_y = np.empty(r, dtype=int)
        for row in range(r):
            activations = self.__fwd_prop(X[row])[-1]
            pred_y[row] = np.argmax(activations)
        return pred_y

    def write_weights(self, filepath):
        '''
        Writes the weights of the neural network to the disk.
        '''
        if len(self.layer_weights) > 0:
            pickle.dump(self.layer_weights, open(filepath,'wb'))
            return true
        else: return false

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description = 'Check backprop gradient \
            computation by comparing with numerically computed gradient ')
    parser.add_argument('data_file', help = 'data file containing feature and \
            labels')
    args = parser.parse_args()
    nnet = network('logistic')
    data = pickle.load(open(args.data_file)) 
    assert nnet.check_gradient(data['X'], data['Y']), 'Incorrect gradient!'

