'''
Module to construct a fully connected neural network.
'''
import numpy as np
from activation_functions import get_actv_func

class network:
    def __init__(self, actv_func):
        self.layer_weights = []
        self.actv, self.actv_der = get_actv_func(actv_func)

    def __random_init(self, n_features, n_classes, hidden_units):
        '''
        Randomly initialize the weights of neural network between [-0.5, 0.5).
        ''' 
        self.layer_weights.append(np.random.rand(hidden_units, n_features) -
                0.5) 

        col_inds = np.arange(hidden_units).reshape((n_classes,-1),
                order='F').reshape((-1,)) 
        row_inds =np.tile(np.arange(n_classes),(hidden_units/n_classes,
                1)).T.reshape(-1) 
        weights = np.zeros((n_classes, hidden_units))
        weights[row_inds, col_inds] = np.random.rand(hidden_units) - 0.5

        self.layer_weights.append(weights)

    def __weights_init(self, layer_weights):
        '''
        Initialize the weights of the neural network with the given weights.
        '''
        self.layer_weights = layer_weights

    def __fwd_prop(self, x, y):
        '''
        Forward propagate the input value with current weights and return
        activations at each stage. The first activation entry is always the
        input itself.
        '''
        activation = [x]
        # calculate activations at hidden layer and output layers
        for ind, l_wt in enumerate(self.layer_weights):
            activation.append(self.actv(np.dot(l_wt, activation[ind]))) 

        return activation
             
    def __back_prop(self, activation, y):
        '''
        Calculates final error and back propogate it to all layers. Use
        activations generated by the forward propogation under the current
        weights. 

        Returns errors on all the hidden layers and output layer in that order.
        '''
        n_layers = len(self.layer_weights) + 1
        errors = [activation[-1] - y]

        # compute errors on hidden layers from o/p to i/p direction
        for ind in range(n_layers - 2):
            layer_ind = -(1 + ind)
            wt = self.layer_weights[layer_ind]
            act = activation[layer_ind - 1]
            next_layer_err = errors[-1]
            errors.append(np.multiply(np.dot(wt.T, next_layer_err),
                self.actv_der(act))) 
        errors.reverse()
        return errors

    def __derivatives(self, x, y):
        '''
        Calculates the partial derivatives at given x and y using fwd and back
        propagation.

        Return partial derivatives for the weight matrices.
        '''
        activation = self.__fwd_prop(x, y)
        deltas = self.__back_prop(activation, y)
        p_derivs = [np.outer(deltas[layer], activation[layer]) for layer in
                range(0, len(activation) - 1)] 
        return p_derivs

    def __update_weights(self, p_derivs, learning_rate):
        '''
        Updates the current weights using the given partial derivatives and the
        learning rate.
        '''
        for layer in range(len(self.layer_weights)): 
            self.layer_weights[layer] -=  p_derivs[layer]

    def __sgd(self, X, Y, epochs = 70000, learning_rate = 1.0):
        '''
        Performs stochastic gradient descent on the dataset X,Y for the given
        number of epochs using the given learning rate.
        '''
        for epoch in range(epochs):
           ind = np.random.randint(0, high=X.shape[0])
           p_derivs = self.__derivatives(X[ind], Y[ind],)
           self.__update_weights(p_derivs, learning_rate)
           if epoch%100 == 0:
               print "Iterations completed: ",epoch

    def train(self, X, Y, hidden_units, layer_weights = None):
        '''
        Trains the network using Stochastic Gradient Descent. Initialize the
        network with the provided layer_weights. If no layer_weights are
        provided use random weights to initialize the network.
        '''
        # add extra feature for bias
        X = np.concatenate((np.ones(X.shape[0])[:, np.newaxis], X), axis=1)
        n_examples, n_features = X.shape
        n_classes = np.unique(Y).size

        # convert Y into a matrix of indicator vector for the class
        new_Y = np.zeros((Y.size, n_classes), dtype=int)
        new_Y[np.array(range(Y.size)), Y] = 1
        Y = new_Y
        
        # initialize network
        if layer_weights == None:self.__random_init(n_features, n_classes,
                hidden_units)
        else: self.__weights_init(layer_weights)

        # train
        self.__sgd(X,Y)
        
    def predict(self, X):
        '''
        Predict the classes based on the learned model and measure accuracy. 
        '''
        r, _ = X.shape
        pred_y = np.empty(r, dtype=int)
        for row in range(r):
            activations = self.__fwd_prop(X[row])[-1]
            pred_y[row] = np.argmax(activations)
        return pred_y

    def write_weights(self, filepath):
        '''
        Writes the weights of the neural network to the disk.
        '''
        if len(self.layer_weights) > 0:
            pickle.dump(self.layer_weights, open(filepath,'wb'))
            return true
        else: return false
        
        
