'''
Module to construct a fully connected neural network.
'''
import numpy as np
from activation_functions import get_actv_func

class network:
    def __init__(self, actv_func):
        self.layer_weights = []
        self.actv, self.actv_der = get_actv_func(actv_func)

    def __random_init(self, n_features, n_classes, hidden_units):
        '''
        Randomly initialize the weights of neural network between [-0.5, 0.5).
        ''' 
        self.layer_weights.append(np.random.rand(self.hl_units, n_features) -
                0.5) 

        col_inds = np.arange(hidden_units).reshape((n_classes,-1),
                order='F').reshape((-1,)) 
        row_inds =np.tile(np.arange(n_classes),(hidden_units/n_classes,
                1)).T.reshape(-1) 
        weights = np.zeros((n_classes, hidden_units))
        weights[row_inds, col_inds] = np.random.rand(hidden_units) - 0.5

        self.layer_weights.append(weights)

    def __weights_init(self, layer_weights):
        '''
        Initialize the weights of the neural network with the given weights.
        '''
        self.layer_weights = layer_weights

    def __fwd_prop(self, x, y):
        '''
        Forward propagate the input value with current weights and return
        activations at each stage.
        '''
        activation = [x]
        # calculate activations at hidden layer and output layers
        for ind, l_wt in enumerate(self.layer_weights):
            activation.append(self.actv(np.dot(activation[ind], l_wt))) 

        return activation[1:]
             
    def __back_prop(self, activation, y):
        '''
        Calculates final error and back propogate it to all layers. Use
        activations generated by the forward propogation under the current
        weights. 

        Returns errors on all the hidden layers and output layer in that order.
        '''
        n_layers = len(self.layer_weights) + 1
        errors = [activation[-1] - y]

        # compute errors on hidden layers from o/p to i/p direction
        for ind in xrange(len(n_layers) - 2):
            layer_ind = -(1 + ind)
            wt = self.layer_weights[layer_ind]
            act = activation[layer_ind]
            next_layer_err = errors[-1]
            errors.append(np.multiply(np.dot(layer_wt, next_layer_err),
                self.actv_der(act))) 
        return reverse(errors)

    def __derivatives(self, x, y):
        '''
        Calculates the partial derivatives at given x and y using fwd and back
        propagation.

        Return partial derivatives for the weight matrices.
        '''
        activation = self.fwd_prop(x, y)
        deltas = self.back_prop(activation, y)
        p_derivs = [np.outer(activation[layer], deltas[layer]) for layer in
                range(0, len(activation))] 
        return p_derivs

    def __update_weights(self, p__derivs, learning_rate):
        '''
        Updates the current weights using the given partial derivatives and the
        learning rate.
        '''
        for layer in range(len(self.layer_weights)): 
            self.layer_weights -= np.multiply(learning_rate, p_derivs[layer])

    def __sgd(self, X, Y, epochs = 70000, learning_rate = 1.0):
        '''
        Performs stochastic gradient descent on the dataset X,Y for the given
        number of epochs using the given learning rate.
        '''
        for epoch in range(epochs):
           ind = numpy.random.randint(0, high=70000)
           p_derivs = self.__derivatives(X[ind], Y[ind],)
           self.__update_weights(p_derivs, learning_rate)

    def train(self, X, Y, hidden_units, layer_weights = None):
        '''
        Trains the network using Stochastic Gradient Descent. Initialize the
        network with the provided layer_weights. If no layer_weights are
        provided use random weights to initialize the network.
        '''
        # add extra feature for bias
        X = np.concatenate((np.ones(X.shape[0])[:, np.newaxis], X), axis=1)
        n_examples, n_features = X.shape
        n_classes = np.unique(Y).size

        # convert Y into a matrix of indicator vector for the class
        new_Y = np.zeros((Y.size, num_classes), dtype=int)
        new_Y[np.array(range(Y.size)), Y] = 1
        Y = new_Y
        
        # initialize network
        if layer_weights == None:self.__random_init(n_features, n_classes,
                hidden_units)
        else: self.__weights_init(layer_weights)

        # train
        self.__sgd(X,Y)
        
    def save_model(self, ddir, suffix = ''):
        '''
        Save the current model to disk in the ddir directory. Additionally add
        any suffix given to the file name; this can be use to give specific
        names to model based on the settings used to create it.
        '''
        pickle.dump(self.layer_weights, open(path.join(ddir, '_', suffix,
            '.model', 'wb')))
            
    def load_model(self, model_file):
        '''
        Load a saved model.
        '''
        self.layer_weights = pickle.load(open(mode_File))

    def predict(self, X):
        '''
        Predict the classes based on the learned model and measure accuracy. 
        '''
        r, _ = X.shape
        pred_y = np.empty(r, dtype=int)
        for row in range(r):
            activations = self.__fwd_prop(X[row])[-1]
            pred_y[row] = np.argmax(activations)
        return pred_y
        
        
